{
  List<InternalAggregation> aggregations=reduceContext.aggregations();
  long globalSubsetSize=0;
  long globalSupersetSize=0;
  for (  InternalAggregation aggregation : aggregations) {
    InternalSignificantTerms terms=(InternalSignificantTerms)aggregation;
    globalSubsetSize+=terms.subsetSize;
    globalSupersetSize+=terms.supersetSize;
  }
  Map<String,List<InternalSignificantTerms.Bucket>> buckets=new HashMap<>();
  for (  InternalAggregation aggregation : aggregations) {
    InternalSignificantTerms terms=(InternalSignificantTerms)aggregation;
    for (    Bucket bucket : terms.buckets) {
      List<Bucket> existingBuckets=buckets.get(bucket.getKey());
      if (existingBuckets == null) {
        existingBuckets=new ArrayList<>(aggregations.size());
        buckets.put(bucket.getKey(),existingBuckets);
      }
      existingBuckets.add(bucket.newBucket(bucket.getSubsetDf(),globalSubsetSize,bucket.getSupersetDf(),globalSupersetSize,bucket.aggregations));
    }
  }
  final int size=Math.min(requiredSize,buckets.size());
  BucketSignificancePriorityQueue ordered=new BucketSignificancePriorityQueue(size);
  for (  Map.Entry<String,List<Bucket>> entry : buckets.entrySet()) {
    List<Bucket> sameTermBuckets=entry.getValue();
    final Bucket b=sameTermBuckets.get(0).reduce(sameTermBuckets,reduceContext);
    b.updateScore(significanceHeuristic);
    if ((b.score > 0) && (b.subsetDf >= minDocCount)) {
      ordered.insertWithOverflow(b);
    }
  }
  Bucket[] list=new Bucket[ordered.size()];
  for (int i=ordered.size() - 1; i >= 0; i--) {
    list[i]=(Bucket)ordered.pop();
  }
  return newAggregation(globalSubsetSize,globalSupersetSize,Arrays.asList(list));
}
