{
  final IndexService indexService=indicesService.indexServiceSafe(request.shardId().index().name());
  final InternalIndexShard shard=(InternalIndexShard)indexService.shardSafe(request.shardId().id());
  RoutingNode node=clusterService.state().readOnlyRoutingNodes().node(request.targetNode().id());
  if (node == null) {
    logger.debug("delaying recovery of {} as source node {} is unknown",request.shardId(),request.targetNode());
    throw new DelayRecoveryException("source node does not have the node [" + request.targetNode() + "] in its state yet..");
  }
  ShardRouting targetShardRouting=null;
  for (  ShardRouting shardRouting : node) {
    if (shardRouting.shardId().equals(request.shardId())) {
      targetShardRouting=shardRouting;
      break;
    }
  }
  if (targetShardRouting == null) {
    logger.debug("delaying recovery of {} as it is not listed as assigned to target node {}",request.shardId(),request.targetNode());
    throw new DelayRecoveryException("source node does not have the shard listed in its state as allocated on the node");
  }
  if (!targetShardRouting.initializing()) {
    logger.debug("delaying recovery of {} as it is not listed as initializing on the target node {}. known shards state is [{}]",request.shardId(),request.targetNode(),targetShardRouting.state());
    throw new DelayRecoveryException("source node has the state of the target shard to be [" + targetShardRouting.state() + "], expecting to be [initializing]");
  }
  logger.trace("[{}][{}] starting recovery to {}, mark_as_relocated {}",request.shardId().index().name(),request.shardId().id(),request.targetNode(),request.markAsRelocated());
  final RecoveryResponse response=new RecoveryResponse();
  shard.recover(new Engine.RecoveryHandler(){
    @Override public void phase1(    final SnapshotIndexCommit snapshot) throws ElasticsearchException {
      long totalSize=0;
      long existingTotalSize=0;
      final Store store=shard.store();
      store.incRef();
      try {
        StopWatch stopWatch=new StopWatch().start();
        for (        String name : snapshot.getFiles()) {
          StoreFileMetaData md=store.metaData(name);
          boolean useExisting=false;
          if (request.existingFiles().containsKey(name)) {
            if (!name.startsWith("segments") && md.isSame(request.existingFiles().get(name))) {
              response.phase1ExistingFileNames.add(name);
              response.phase1ExistingFileSizes.add(md.length());
              existingTotalSize+=md.length();
              useExisting=true;
              if (logger.isTraceEnabled()) {
                logger.trace("[{}][{}] recovery [phase1] to {}: not recovering [{}], exists in local store and has checksum [{}], size [{}]",request.shardId().index().name(),request.shardId().id(),request.targetNode(),name,md.checksum(),md.length());
              }
            }
          }
          if (!useExisting) {
            if (request.existingFiles().containsKey(name)) {
              logger.trace("[{}][{}] recovery [phase1] to {}: recovering [{}], exists in local store, but is different: remote [{}], local [{}]",request.shardId().index().name(),request.shardId().id(),request.targetNode(),name,request.existingFiles().get(name),md);
            }
 else {
              logger.trace("[{}][{}] recovery [phase1] to {}: recovering [{}], does not exists in remote",request.shardId().index().name(),request.shardId().id(),request.targetNode(),name);
            }
            response.phase1FileNames.add(name);
            response.phase1FileSizes.add(md.length());
          }
          totalSize+=md.length();
        }
        response.phase1TotalSize=totalSize;
        response.phase1ExistingTotalSize=existingTotalSize;
        logger.trace("[{}][{}] recovery [phase1] to {}: recovering_files [{}] with total_size [{}], reusing_files [{}] with total_size [{}]",request.shardId().index().name(),request.shardId().id(),request.targetNode(),response.phase1FileNames.size(),new ByteSizeValue(totalSize),response.phase1ExistingFileNames.size(),new ByteSizeValue(existingTotalSize));
        RecoveryFilesInfoRequest recoveryInfoFilesRequest=new RecoveryFilesInfoRequest(request.recoveryId(),request.shardId(),response.phase1FileNames,response.phase1FileSizes,response.phase1ExistingFileNames,response.phase1ExistingFileSizes,response.phase1TotalSize,response.phase1ExistingTotalSize);
        transportService.submitRequest(request.targetNode(),RecoveryTarget.Actions.FILES_INFO,recoveryInfoFilesRequest,TransportRequestOptions.options().withTimeout(internalActionTimeout),EmptyTransportResponseHandler.INSTANCE_SAME).txGet();
        final CountDownLatch latch=new CountDownLatch(response.phase1FileNames.size());
        final AtomicReference<Throwable> lastException=new AtomicReference<>();
        int fileIndex=0;
        for (        final String name : response.phase1FileNames) {
          ThreadPoolExecutor pool;
          long fileSize=response.phase1FileSizes.get(fileIndex);
          if (fileSize > recoverySettings.SMALL_FILE_CUTOFF_BYTES) {
            pool=recoverySettings.concurrentStreamPool();
          }
 else {
            pool=recoverySettings.concurrentSmallFileStreamPool();
          }
          pool.execute(new Runnable(){
            @Override public void run(){
              IndexInput indexInput=null;
              store.incRef();
              try {
                final int BUFFER_SIZE=(int)recoverySettings.fileChunkSize().bytes();
                byte[] buf=new byte[BUFFER_SIZE];
                StoreFileMetaData md=store.metaData(name);
                indexInput=store.openInputRaw(name,IOContext.READ);
                boolean shouldCompressRequest=recoverySettings.compress();
                if (CompressorFactory.isCompressed(indexInput)) {
                  shouldCompressRequest=false;
                }
                long len=indexInput.length();
                long readCount=0;
                while (readCount < len) {
                  if (shard.state() == IndexShardState.CLOSED) {
                    throw new IndexShardClosedException(shard.shardId());
                  }
                  int toRead=readCount + BUFFER_SIZE > len ? (int)(len - readCount) : BUFFER_SIZE;
                  long position=indexInput.getFilePointer();
                  if (recoverySettings.rateLimiter() != null) {
                    recoverySettings.rateLimiter().pause(toRead);
                  }
                  indexInput.readBytes(buf,0,toRead,false);
                  BytesArray content=new BytesArray(buf,0,toRead);
                  transportService.submitRequest(request.targetNode(),RecoveryTarget.Actions.FILE_CHUNK,new RecoveryFileChunkRequest(request.recoveryId(),request.shardId(),name,position,len,md.checksum(),content),TransportRequestOptions.options().withCompress(shouldCompressRequest).withType(TransportRequestOptions.Type.RECOVERY).withTimeout(internalActionTimeout),EmptyTransportResponseHandler.INSTANCE_SAME).txGet();
                  readCount+=toRead;
                }
              }
 catch (              Throwable e) {
                lastException.set(e);
              }
 finally {
                IOUtils.closeWhileHandlingException(indexInput);
                try {
                  store.decRef();
                }
  finally {
                  latch.countDown();
                }
              }
            }
          }
);
          fileIndex++;
        }
        latch.await();
        if (lastException.get() != null) {
          throw lastException.get();
        }
        Set<String> snapshotFiles=Sets.newHashSet(snapshot.getFiles());
        transportService.submitRequest(request.targetNode(),RecoveryTarget.Actions.CLEAN_FILES,new RecoveryCleanFilesRequest(request.recoveryId(),shard.shardId(),snapshotFiles),TransportRequestOptions.options().withTimeout(internalActionTimeout),EmptyTransportResponseHandler.INSTANCE_SAME).txGet();
        stopWatch.stop();
        logger.trace("[{}][{}] recovery [phase1] to {}: took [{}]",request.shardId().index().name(),request.shardId().id(),request.targetNode(),stopWatch.totalTime());
        response.phase1Time=stopWatch.totalTime().millis();
      }
 catch (      Throwable e) {
        throw new RecoverFilesRecoveryException(request.shardId(),response.phase1FileNames.size(),new ByteSizeValue(totalSize),e);
      }
 finally {
        store.decRef();
      }
    }
    @Override public void phase2(    Translog.Snapshot snapshot) throws ElasticsearchException {
      if (shard.state() == IndexShardState.CLOSED) {
        throw new IndexShardClosedException(request.shardId());
      }
      logger.trace("{} recovery [phase2] to {}: start",request.shardId(),request.targetNode());
      StopWatch stopWatch=new StopWatch().start();
      transportService.submitRequest(request.targetNode(),RecoveryTarget.Actions.PREPARE_TRANSLOG,new RecoveryPrepareForTranslogOperationsRequest(request.recoveryId(),request.shardId()),TransportRequestOptions.options().withTimeout(internalActionTimeout),EmptyTransportResponseHandler.INSTANCE_SAME).txGet();
      stopWatch.stop();
      response.startTime=stopWatch.totalTime().millis();
      logger.trace("{} recovery [phase2] to {}: start took [{}]",request.shardId(),request.targetNode(),request.targetNode(),stopWatch.totalTime());
      logger.trace("{} recovery [phase2] to {}: updating current mapping to master",request.shardId(),request.targetNode());
      updateMappingOnMaster();
      logger.trace("{} recovery [phase2] to {}: sending transaction log operations",request.shardId(),request.targetNode());
      stopWatch=new StopWatch().start();
      int totalOperations=sendSnapshot(snapshot);
      stopWatch.stop();
      logger.trace("{} recovery [phase2] to {}: took [{}]",request.shardId(),request.targetNode(),stopWatch.totalTime());
      response.phase2Time=stopWatch.totalTime().millis();
      response.phase2Operations=totalOperations;
    }
    private void updateMappingOnMaster(){
      List<DocumentMapper> documentMappersToUpdate=Lists.newArrayList();
      for (      DocumentMapper documentMapper : indexService.mapperService()) {
        if (documentMapper.type().equals(MapperService.DEFAULT_MAPPING)) {
          continue;
        }
        documentMappersToUpdate.add(documentMapper);
      }
      if (documentMappersToUpdate.isEmpty()) {
        return;
      }
      final CountDownLatch countDownLatch=new CountDownLatch(documentMappersToUpdate.size());
      MappingUpdatedAction.MappingUpdateListener listener=new MappingUpdatedAction.MappingUpdateListener(){
        @Override public void onMappingUpdate(){
          countDownLatch.countDown();
        }
        @Override public void onFailure(        Throwable t){
          logger.debug("{} recovery to {}: failed to update mapping on master",request.shardId(),request.targetNode(),t);
          countDownLatch.countDown();
        }
      }
;
      for (      DocumentMapper documentMapper : documentMappersToUpdate) {
        mappingUpdatedAction.updateMappingOnMaster(indexService.index().getName(),documentMapper,indexService.indexUUID(),listener);
      }
      try {
        if (!countDownLatch.await(internalActionTimeout.millis(),TimeUnit.MILLISECONDS)) {
          logger.debug("{} recovery [phase2] to {}: waiting on pending mapping update timed out. waited [{}]",request.shardId(),request.targetNode(),internalActionTimeout);
        }
      }
 catch (      InterruptedException e) {
        Thread.currentThread().interrupt();
        logger.debug("interrupted while waiting for mapping to update on master");
      }
    }
    @Override public void phase3(    Translog.Snapshot snapshot) throws ElasticsearchException {
      if (shard.state() == IndexShardState.CLOSED) {
        throw new IndexShardClosedException(request.shardId());
      }
      logger.trace("[{}][{}] recovery [phase3] to {}: sending transaction log operations",request.shardId().index().name(),request.shardId().id(),request.targetNode());
      StopWatch stopWatch=new StopWatch().start();
      int totalOperations=sendSnapshot(snapshot);
      transportService.submitRequest(request.targetNode(),RecoveryTarget.Actions.FINALIZE,new RecoveryFinalizeRecoveryRequest(request.recoveryId(),request.shardId()),TransportRequestOptions.options().withTimeout(internalActionLongTimeout),EmptyTransportResponseHandler.INSTANCE_SAME).txGet();
      if (request.markAsRelocated()) {
        try {
          shard.relocated("to " + request.targetNode());
        }
 catch (        IllegalIndexShardStateException e) {
        }
      }
      stopWatch.stop();
      logger.trace("[{}][{}] recovery [phase3] to {}: took [{}]",request.shardId().index().name(),request.shardId().id(),request.targetNode(),stopWatch.totalTime());
      response.phase3Time=stopWatch.totalTime().millis();
      response.phase3Operations=totalOperations;
    }
    private int sendSnapshot(    Translog.Snapshot snapshot) throws ElasticsearchException {
      int ops=0;
      long size=0;
      int totalOperations=0;
      List<Translog.Operation> operations=Lists.newArrayList();
      while (snapshot.hasNext()) {
        if (shard.state() == IndexShardState.CLOSED) {
          throw new IndexShardClosedException(request.shardId());
        }
        Translog.Operation operation=snapshot.next();
        operations.add(operation);
        ops+=1;
        size+=operation.estimateSize();
        totalOperations++;
        if (ops >= recoverySettings.translogOps() || size >= recoverySettings.translogSize().bytes()) {
          RecoveryTranslogOperationsRequest translogOperationsRequest=new RecoveryTranslogOperationsRequest(request.recoveryId(),request.shardId(),operations);
          transportService.submitRequest(request.targetNode(),RecoveryTarget.Actions.TRANSLOG_OPS,translogOperationsRequest,TransportRequestOptions.options().withCompress(recoverySettings.compress()).withType(TransportRequestOptions.Type.RECOVERY).withTimeout(internalActionLongTimeout),EmptyTransportResponseHandler.INSTANCE_SAME).txGet();
          ops=0;
          size=0;
          operations.clear();
        }
      }
      if (!operations.isEmpty()) {
        RecoveryTranslogOperationsRequest translogOperationsRequest=new RecoveryTranslogOperationsRequest(request.recoveryId(),request.shardId(),operations);
        transportService.submitRequest(request.targetNode(),RecoveryTarget.Actions.TRANSLOG_OPS,translogOperationsRequest,TransportRequestOptions.options().withCompress(recoverySettings.compress()).withType(TransportRequestOptions.Type.RECOVERY).withTimeout(internalActionLongTimeout),EmptyTransportResponseHandler.INSTANCE_SAME).txGet();
      }
      return totalOperations;
    }
  }
);
  return response;
}
