{
  final IndexService indexService=indicesService.indexServiceSafe(request.shardId().index().name());
  final InternalIndexShard shard=(InternalIndexShard)indexService.shardSafe(request.shardId().id());
  RoutingNode node=clusterService.state().readOnlyRoutingNodes().node(request.targetNode().id());
  if (node == null) {
    logger.debug("delaying recovery of {} as source node {} is unknown",request.shardId(),request.targetNode());
    throw new DelayRecoveryException("source node does not have the node [" + request.targetNode() + "] in its state yet..");
  }
  ShardRouting targetShardRouting=null;
  for (  ShardRouting shardRouting : node) {
    if (shardRouting.shardId().equals(request.shardId())) {
      targetShardRouting=shardRouting;
      break;
    }
  }
  if (targetShardRouting == null) {
    logger.debug("delaying recovery of {} as it is not listed as assigned to target node {}",request.shardId(),request.targetNode());
    throw new DelayRecoveryException("source node does not have the shard listed in its state as allocated on the node");
  }
  if (!targetShardRouting.initializing()) {
    logger.debug("delaying recovery of {} as it is not listed as initializing on the target node {}. known shards state is [{}]",request.shardId(),request.targetNode(),targetShardRouting.state());
    throw new DelayRecoveryException("source node has the state of the target shard to be [" + targetShardRouting.state() + "], expecting to be [initializing]");
  }
  logger.trace("[{}][{}] starting recovery to {}, mark_as_relocated {}",request.shardId().index().name(),request.shardId().id(),request.targetNode(),request.markAsRelocated());
  final RecoveryResponse response=new RecoveryResponse();
  shard.recover(new Engine.RecoveryHandler(){
    @Override public void phase1(    final SnapshotIndexCommit snapshot) throws ElasticsearchException {
      long totalSize=0;
      long existingTotalSize=0;
      final Store store=shard.store();
      store.incRef();
      try {
        StopWatch stopWatch=new StopWatch().start();
        final Store.MetadataSnapshot recoverySourceMetadata=store.getMetadata(snapshot);
        for (        String name : snapshot.getFiles()) {
          final StoreFileMetaData md=recoverySourceMetadata.get(name);
          if (md == null) {
            logger.info("Snapshot differs from actual index for file: {} meta: {}",name,recoverySourceMetadata.asMap());
            throw new CorruptIndexException("Snapshot differs from actual index - maybe index was removed metadata has " + recoverySourceMetadata.asMap().size() + " files",name);
          }
        }
        final Store.RecoveryDiff diff=recoverySourceMetadata.recoveryDiff(new Store.MetadataSnapshot(request.existingFiles()));
        for (        StoreFileMetaData md : diff.identical) {
          response.phase1ExistingFileNames.add(md.name());
          response.phase1ExistingFileSizes.add(md.length());
          existingTotalSize+=md.length();
          if (logger.isTraceEnabled()) {
            logger.trace("[{}][{}] recovery [phase1] to {}: not recovering [{}], exists in local store and has checksum [{}], size [{}]",request.shardId().index().name(),request.shardId().id(),request.targetNode(),md.name(),md.checksum(),md.length());
          }
          totalSize+=md.length();
        }
        for (        StoreFileMetaData md : Iterables.concat(diff.different,diff.missing)) {
          if (request.existingFiles().containsKey(md.name())) {
            logger.trace("[{}][{}] recovery [phase1] to {}: recovering [{}], exists in local store, but is different: remote [{}], local [{}]",request.shardId().index().name(),request.shardId().id(),request.targetNode(),md.name(),request.existingFiles().get(md.name()),md);
          }
 else {
            logger.trace("[{}][{}] recovery [phase1] to {}: recovering [{}], does not exists in remote",request.shardId().index().name(),request.shardId().id(),request.targetNode(),md.name());
          }
          response.phase1FileNames.add(md.name());
          response.phase1FileSizes.add(md.length());
          totalSize+=md.length();
        }
        response.phase1TotalSize=totalSize;
        response.phase1ExistingTotalSize=existingTotalSize;
        logger.trace("[{}][{}] recovery [phase1] to {}: recovering_files [{}] with total_size [{}], reusing_files [{}] with total_size [{}]",request.shardId().index().name(),request.shardId().id(),request.targetNode(),response.phase1FileNames.size(),new ByteSizeValue(totalSize),response.phase1ExistingFileNames.size(),new ByteSizeValue(existingTotalSize));
        RecoveryFilesInfoRequest recoveryInfoFilesRequest=new RecoveryFilesInfoRequest(request.recoveryId(),request.shardId(),response.phase1FileNames,response.phase1FileSizes,response.phase1ExistingFileNames,response.phase1ExistingFileSizes,response.phase1TotalSize,response.phase1ExistingTotalSize);
        transportService.submitRequest(request.targetNode(),RecoveryTarget.Actions.FILES_INFO,recoveryInfoFilesRequest,TransportRequestOptions.options().withTimeout(internalActionTimeout),EmptyTransportResponseHandler.INSTANCE_SAME).txGet();
        final CountDownLatch latch=new CountDownLatch(response.phase1FileNames.size());
        final CopyOnWriteArrayList<Throwable> exceptions=new CopyOnWriteArrayList<>();
        final AtomicReference<Throwable> corruptedEngine=new AtomicReference<>();
        int fileIndex=0;
        for (        final String name : response.phase1FileNames) {
          ThreadPoolExecutor pool;
          long fileSize=response.phase1FileSizes.get(fileIndex);
          if (fileSize > recoverySettings.SMALL_FILE_CUTOFF_BYTES) {
            pool=recoverySettings.concurrentStreamPool();
          }
 else {
            pool=recoverySettings.concurrentSmallFileStreamPool();
          }
          pool.execute(new Runnable(){
            @Override public void run(){
              store.incRef();
              final StoreFileMetaData md=recoverySourceMetadata.get(name);
              try (final IndexInput indexInput=store.directory().openInput(name,IOContext.READONCE)){
                final int BUFFER_SIZE=(int)recoverySettings.fileChunkSize().bytes();
                final byte[] buf=new byte[BUFFER_SIZE];
                boolean shouldCompressRequest=recoverySettings.compress();
                if (CompressorFactory.isCompressed(indexInput)) {
                  shouldCompressRequest=false;
                }
                long len=indexInput.length();
                long readCount=0;
                while (readCount < len) {
                  if (shard.state() == IndexShardState.CLOSED) {
                    throw new IndexShardClosedException(shard.shardId());
                  }
                  int toRead=readCount + BUFFER_SIZE > len ? (int)(len - readCount) : BUFFER_SIZE;
                  long position=indexInput.getFilePointer();
                  if (recoverySettings.rateLimiter() != null) {
                    recoverySettings.rateLimiter().pause(toRead);
                  }
                  indexInput.readBytes(buf,0,toRead,false);
                  BytesArray content=new BytesArray(buf,0,toRead);
                  readCount+=toRead;
                  transportService.submitRequest(request.targetNode(),RecoveryTarget.Actions.FILE_CHUNK,new RecoveryFileChunkRequest(request.recoveryId(),request.shardId(),md,position,content,readCount == len),TransportRequestOptions.options().withCompress(shouldCompressRequest).withType(TransportRequestOptions.Type.RECOVERY).withTimeout(internalActionTimeout),EmptyTransportResponseHandler.INSTANCE_SAME).txGet();
                }
              }
 catch (              Throwable e) {
                final Throwable corruptIndexException;
                if ((corruptIndexException=ExceptionsHelper.unwrapCorruption(e)) != null) {
                  if (store.checkIntegrity(md) == false) {
                    logger.warn("{} Corrupted file detected {} checksum mismatch",shard.shardId(),md);
                    if (corruptedEngine.compareAndSet(null,corruptIndexException) == false) {
                      corruptedEngine.get().addSuppressed(e);
                    }
                  }
 else {
                    RemoteTransportException exception=new RemoteTransportException("File corruption occured on recovery but checksums are ok",null);
                    exception.addSuppressed(e);
                    exceptions.add(0,exception);
                    logger.warn("{} File corruption on recovery {} local checksum OK",corruptIndexException,shard.shardId(),md);
                  }
                }
 else {
                  exceptions.add(0,e);
                }
              }
 finally {
                try {
                  store.decRef();
                }
  finally {
                  latch.countDown();
                }
              }
            }
          }
);
          fileIndex++;
        }
        latch.await();
        if (corruptedEngine.get() != null) {
          throw corruptedEngine.get();
        }
 else {
          ExceptionsHelper.rethrowAndSuppress(exceptions);
        }
        Set<String> snapshotFiles=Sets.newHashSet(snapshot.getFiles());
        transportService.submitRequest(request.targetNode(),RecoveryTarget.Actions.CLEAN_FILES,new RecoveryCleanFilesRequest(request.recoveryId(),shard.shardId(),snapshotFiles),TransportRequestOptions.options().withTimeout(internalActionTimeout),EmptyTransportResponseHandler.INSTANCE_SAME).txGet();
        stopWatch.stop();
        logger.trace("[{}][{}] recovery [phase1] to {}: took [{}]",request.shardId().index().name(),request.shardId().id(),request.targetNode(),stopWatch.totalTime());
        response.phase1Time=stopWatch.totalTime().millis();
      }
 catch (      Throwable e) {
        throw new RecoverFilesRecoveryException(request.shardId(),response.phase1FileNames.size(),new ByteSizeValue(totalSize),e);
      }
 finally {
        store.decRef();
      }
    }
    @Override public void phase2(    Translog.Snapshot snapshot) throws ElasticsearchException {
      if (shard.state() == IndexShardState.CLOSED) {
        throw new IndexShardClosedException(request.shardId());
      }
      logger.trace("{} recovery [phase2] to {}: start",request.shardId(),request.targetNode());
      StopWatch stopWatch=new StopWatch().start();
      transportService.submitRequest(request.targetNode(),RecoveryTarget.Actions.PREPARE_TRANSLOG,new RecoveryPrepareForTranslogOperationsRequest(request.recoveryId(),request.shardId()),TransportRequestOptions.options().withTimeout(internalActionTimeout),EmptyTransportResponseHandler.INSTANCE_SAME).txGet();
      stopWatch.stop();
      response.startTime=stopWatch.totalTime().millis();
      logger.trace("{} recovery [phase2] to {}: start took [{}]",request.shardId(),request.targetNode(),request.targetNode(),stopWatch.totalTime());
      logger.trace("{} recovery [phase2] to {}: updating current mapping to master",request.shardId(),request.targetNode());
      updateMappingOnMaster();
      logger.trace("{} recovery [phase2] to {}: sending transaction log operations",request.shardId(),request.targetNode());
      stopWatch=new StopWatch().start();
      int totalOperations=sendSnapshot(snapshot);
      stopWatch.stop();
      logger.trace("{} recovery [phase2] to {}: took [{}]",request.shardId(),request.targetNode(),stopWatch.totalTime());
      response.phase2Time=stopWatch.totalTime().millis();
      response.phase2Operations=totalOperations;
    }
    private void updateMappingOnMaster(){
      final BlockingQueue<DocumentMapper> documentMappersToUpdate=ConcurrentCollections.newBlockingQueue();
      final CountDownLatch latch=new CountDownLatch(1);
      clusterService.submitStateUpdateTask("recovery_mapping_check",new ProcessedClusterStateNonMasterUpdateTask(){
        @Override public void clusterStateProcessed(        String source,        ClusterState oldState,        ClusterState newState){
          latch.countDown();
        }
        @Override public ClusterState execute(        ClusterState currentState) throws Exception {
          IndexMetaData indexMetaData=clusterService.state().metaData().getIndices().get(indexService.index().getName());
          ImmutableOpenMap<String,MappingMetaData> metaDataMappings=null;
          if (indexMetaData != null) {
            metaDataMappings=indexMetaData.getMappings();
          }
          for (          DocumentMapper documentMapper : indexService.mapperService().docMappers(false)) {
            MappingMetaData mappingMetaData=metaDataMappings == null ? null : metaDataMappings.get(documentMapper.type());
            if (mappingMetaData == null || !documentMapper.refreshSource().equals(mappingMetaData.source())) {
              documentMappersToUpdate.add(documentMapper);
            }
          }
          return currentState;
        }
        @Override public void onFailure(        String source,        @Nullable Throwable t){
          logger.error("unexpected error while checking for pending mapping changes",t);
          latch.countDown();
        }
      }
);
      try {
        latch.await();
      }
 catch (      InterruptedException e) {
        Thread.currentThread().interrupt();
      }
      if (documentMappersToUpdate.isEmpty()) {
        return;
      }
      final CountDownLatch updatedOnMaster=new CountDownLatch(documentMappersToUpdate.size());
      MappingUpdatedAction.MappingUpdateListener listener=new MappingUpdatedAction.MappingUpdateListener(){
        @Override public void onMappingUpdate(){
          updatedOnMaster.countDown();
        }
        @Override public void onFailure(        Throwable t){
          logger.debug("{} recovery to {}: failed to update mapping on master",request.shardId(),request.targetNode(),t);
          updatedOnMaster.countDown();
        }
      }
;
      for (      DocumentMapper documentMapper : documentMappersToUpdate) {
        mappingUpdatedAction.updateMappingOnMaster(indexService.index().getName(),documentMapper,indexService.indexUUID(),listener);
      }
      try {
        if (!updatedOnMaster.await(internalActionTimeout.millis(),TimeUnit.MILLISECONDS)) {
          logger.debug("{} recovery [phase2] to {}: waiting on pending mapping update timed out. waited [{}]",request.shardId(),request.targetNode(),internalActionTimeout);
        }
      }
 catch (      InterruptedException e) {
        Thread.currentThread().interrupt();
        logger.debug("interrupted while waiting for mapping to update on master");
      }
    }
    @Override public void phase3(    Translog.Snapshot snapshot) throws ElasticsearchException {
      if (shard.state() == IndexShardState.CLOSED) {
        throw new IndexShardClosedException(request.shardId());
      }
      logger.trace("[{}][{}] recovery [phase3] to {}: sending transaction log operations",request.shardId().index().name(),request.shardId().id(),request.targetNode());
      StopWatch stopWatch=new StopWatch().start();
      int totalOperations=sendSnapshot(snapshot);
      transportService.submitRequest(request.targetNode(),RecoveryTarget.Actions.FINALIZE,new RecoveryFinalizeRecoveryRequest(request.recoveryId(),request.shardId()),TransportRequestOptions.options().withTimeout(internalActionLongTimeout),EmptyTransportResponseHandler.INSTANCE_SAME).txGet();
      if (request.markAsRelocated()) {
        try {
          shard.relocated("to " + request.targetNode());
        }
 catch (        IllegalIndexShardStateException e) {
        }
      }
      stopWatch.stop();
      logger.trace("[{}][{}] recovery [phase3] to {}: took [{}]",request.shardId().index().name(),request.shardId().id(),request.targetNode(),stopWatch.totalTime());
      response.phase3Time=stopWatch.totalTime().millis();
      response.phase3Operations=totalOperations;
    }
    private int sendSnapshot(    Translog.Snapshot snapshot) throws ElasticsearchException {
      int ops=0;
      long size=0;
      int totalOperations=0;
      List<Translog.Operation> operations=Lists.newArrayList();
      Translog.Operation operation=snapshot.next();
      while (operation != null) {
        if (shard.state() == IndexShardState.CLOSED) {
          throw new IndexShardClosedException(request.shardId());
        }
        operations.add(operation);
        ops+=1;
        size+=operation.estimateSize();
        totalOperations++;
        if (ops >= recoverySettings.translogOps() || size >= recoverySettings.translogSize().bytes()) {
          RecoveryTranslogOperationsRequest translogOperationsRequest=new RecoveryTranslogOperationsRequest(request.recoveryId(),request.shardId(),operations);
          transportService.submitRequest(request.targetNode(),RecoveryTarget.Actions.TRANSLOG_OPS,translogOperationsRequest,TransportRequestOptions.options().withCompress(recoverySettings.compress()).withType(TransportRequestOptions.Type.RECOVERY).withTimeout(internalActionLongTimeout),EmptyTransportResponseHandler.INSTANCE_SAME).txGet();
          ops=0;
          size=0;
          operations.clear();
        }
        operation=snapshot.next();
      }
      if (!operations.isEmpty()) {
        RecoveryTranslogOperationsRequest translogOperationsRequest=new RecoveryTranslogOperationsRequest(request.recoveryId(),request.shardId(),operations);
        transportService.submitRequest(request.targetNode(),RecoveryTarget.Actions.TRANSLOG_OPS,translogOperationsRequest,TransportRequestOptions.options().withCompress(recoverySettings.compress()).withType(TransportRequestOptions.Type.RECOVERY).withTimeout(internalActionLongTimeout),EmptyTransportResponseHandler.INSTANCE_SAME).txGet();
      }
      return totalOperations;
    }
  }
);
  return response;
}
