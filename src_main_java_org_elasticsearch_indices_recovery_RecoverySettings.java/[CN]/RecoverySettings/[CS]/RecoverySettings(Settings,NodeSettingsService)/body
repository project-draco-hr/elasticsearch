{
  super(settings);
  this.fileChunkSize=componentSettings.getAsBytesSize("file_chunk_size",settings.getAsBytesSize("index.shard.recovery.file_chunk_size",new ByteSizeValue(512,ByteSizeUnit.KB)));
  this.translogOps=componentSettings.getAsInt("translog_ops",settings.getAsInt("index.shard.recovery.translog_ops",1000));
  this.translogSize=componentSettings.getAsBytesSize("translog_size",settings.getAsBytesSize("index.shard.recovery.translog_size",new ByteSizeValue(512,ByteSizeUnit.KB)));
  this.compress=componentSettings.getAsBoolean("compress",true);
  this.concurrentStreams=componentSettings.getAsInt("concurrent_streams",settings.getAsInt("index.shard.recovery.concurrent_streams",3));
  this.concurrentStreamPool=EsExecutors.newScaling(0,concurrentStreams,60,TimeUnit.SECONDS,EsExecutors.daemonThreadFactory(settings,"[recovery_stream]"));
  this.concurrentSmallFileStreams=componentSettings.getAsInt("concurrent_small_file_streams",settings.getAsInt("index.shard.recovery.concurrent_small_file_streams",2));
  this.concurrentSmallFileStreamPool=EsExecutors.newScaling(0,concurrentSmallFileStreams,60,TimeUnit.SECONDS,EsExecutors.daemonThreadFactory(settings,"[small_file_recovery_stream]"));
  this.maxBytesPerSec=componentSettings.getAsBytesSize("max_bytes_per_sec",componentSettings.getAsBytesSize("max_size_per_sec",new ByteSizeValue(50,ByteSizeUnit.MB)));
  if (maxBytesPerSec.bytes() <= 0) {
    rateLimiter=null;
  }
 else {
    rateLimiter=new SimpleRateLimiter(maxBytesPerSec.mbFrac());
  }
  logger.debug("using max_bytes_per_sec[{}], concurrent_streams [{}], file_chunk_size [{}], translog_size [{}], translog_ops [{}], and compress [{}]",maxBytesPerSec,concurrentStreams,fileChunkSize,translogSize,translogOps,compress);
  nodeSettingsService.addListener(new ApplySettings());
}
