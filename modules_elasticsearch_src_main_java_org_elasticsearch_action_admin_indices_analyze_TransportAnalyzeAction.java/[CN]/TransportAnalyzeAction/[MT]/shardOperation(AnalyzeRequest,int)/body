{
  IndexService indexService=indicesService.indexServiceSafe(request.index());
  Analyzer analyzer=null;
  String field="contents";
  String dtype=null;
  if (request.field() != null)   field=request.field();
  if (request.type() != null)   dtype=request.type();
  if (request.field() != null || request.type() != null) {
    final DocumentMapper mapper=indexService.mapperService().documentMapper(dtype);
    if (mapper != null) {
      analyzer=mapper.mappers().indexAnalyzer();
    }
  }
  if (analyzer == null && request.analyzer() != null) {
    analyzer=indexService.analysisService().analyzer(request.analyzer());
  }
 else   if (analyzer == null) {
    analyzer=indexService.analysisService().defaultIndexAnalyzer();
  }
  if (analyzer == null) {
    throw new ElasticSearchIllegalArgumentException("failed to find analyzer");
  }
  List<AnalyzeResponse.AnalyzeToken> tokens=Lists.newArrayList();
  TokenStream stream=null;
  try {
    stream=analyzer.reusableTokenStream(field,new FastStringReader(request.text()));
    stream.reset();
    CharTermAttribute term=stream.addAttribute(CharTermAttribute.class);
    PositionIncrementAttribute posIncr=stream.addAttribute(PositionIncrementAttribute.class);
    OffsetAttribute offset=stream.addAttribute(OffsetAttribute.class);
    TypeAttribute type=stream.addAttribute(TypeAttribute.class);
    int position=0;
    while (stream.incrementToken()) {
      int increment=posIncr.getPositionIncrement();
      if (increment > 0) {
        position=position + increment;
      }
      tokens.add(new AnalyzeResponse.AnalyzeToken(term.toString(),position,offset.startOffset(),offset.endOffset(),type.type()));
    }
    stream.end();
  }
 catch (  IOException e) {
    throw new ElasticSearchException("failed to analyze",e);
  }
 finally {
    if (stream != null) {
      try {
        stream.close();
      }
 catch (      IOException e) {
      }
    }
  }
  return new AnalyzeResponse(tokens);
}
