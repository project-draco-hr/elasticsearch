{
  super(index,indexSettings,name,settings);
  List<String> rules=Analysis.getWordList(env,settings,"synonyms");
  if (rules == null) {
    throw new ElasticSearchIllegalArgumentException("synonym requires either `synonyms` or `synonyms_path` to be configured");
  }
  this.ignoreCase=settings.getAsBoolean("ignore_case",false);
  boolean expand=settings.getAsBoolean("expand",true);
  String tokenizerName=settings.get("tokenizer","whitespace");
  TokenizerFactoryFactory tokenizerFactoryFactory=tokenizerFactories.get(tokenizerName);
  if (tokenizerFactoryFactory == null) {
    tokenizerFactoryFactory=indicesAnalysisService.tokenizerFactoryFactory(tokenizerName);
  }
  if (tokenizerFactoryFactory == null) {
    throw new ElasticSearchIllegalArgumentException("failed to fine tokenizer [" + tokenizerName + "] for synonym token filter");
  }
  final TokenizerFactory tokenizerFactory=tokenizerFactoryFactory.create(tokenizerName,settings);
  Analyzer analyzer=new ReusableAnalyzerBase(){
    @Override protected TokenStreamComponents createComponents(    String fieldName,    Reader reader){
      Tokenizer tokenizer=tokenizerFactory == null ? new WhitespaceTokenizer(Lucene.ANALYZER_VERSION,reader) : tokenizerFactory.create(reader);
      TokenStream stream=ignoreCase ? new LowerCaseFilter(Lucene.ANALYZER_VERSION,tokenizer) : tokenizer;
      return new TokenStreamComponents(tokenizer,stream);
    }
  }
;
  CustomSynonymParser parser=new CustomSynonymParser(true,expand,analyzer);
  try {
    synonymMap=parser.build();
  }
 catch (  IOException e) {
    throw new ElasticSearchIllegalArgumentException("failed to build synonyms",e);
  }
}
