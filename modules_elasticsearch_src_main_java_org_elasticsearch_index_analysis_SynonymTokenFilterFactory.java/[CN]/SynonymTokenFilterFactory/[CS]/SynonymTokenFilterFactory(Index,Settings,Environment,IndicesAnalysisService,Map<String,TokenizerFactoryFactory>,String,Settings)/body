{
  super(index,indexSettings,name,settings);
  Reader rulesReader=Analysis.getFileReader(env,settings,"synonyms");
  if (rulesReader == null) {
    throw new ElasticSearchIllegalArgumentException("synonym requires either `synonyms` or `synonyms_path` to be configured");
  }
  this.ignoreCase=settings.getAsBoolean("ignore_case",false);
  boolean expand=settings.getAsBoolean("expand",true);
  String tokenizerName=settings.get("tokenizer","whitespace");
  TokenizerFactoryFactory tokenizerFactoryFactory=tokenizerFactories.get(tokenizerName);
  if (tokenizerFactoryFactory == null) {
    tokenizerFactoryFactory=indicesAnalysisService.tokenizerFactoryFactory(tokenizerName);
  }
  if (tokenizerFactoryFactory == null) {
    throw new ElasticSearchIllegalArgumentException("failed to find tokenizer [" + tokenizerName + "] for synonym token filter");
  }
  final TokenizerFactory tokenizerFactory=tokenizerFactoryFactory.create(tokenizerName,settings);
  Analyzer analyzer=new ReusableAnalyzerBase(){
    @Override protected TokenStreamComponents createComponents(    String fieldName,    Reader reader){
      Tokenizer tokenizer=tokenizerFactory == null ? new WhitespaceTokenizer(Lucene.ANALYZER_VERSION,reader) : tokenizerFactory.create(reader);
      TokenStream stream=ignoreCase ? new LowerCaseFilter(Lucene.ANALYZER_VERSION,tokenizer) : tokenizer;
      return new TokenStreamComponents(tokenizer,stream);
    }
  }
;
  try {
    SynonymMap.Builder parser=null;
    if (settings.get("format","wordnet").equalsIgnoreCase("wordnet")) {
      parser=new WordnetSynonymParser(true,expand,analyzer);
      ((WordnetSynonymParser)parser).add(rulesReader);
    }
 else {
      parser=new SolrSynonymParser(true,expand,analyzer);
      ((SolrSynonymParser)parser).add(rulesReader);
    }
    synonymMap=parser.build();
  }
 catch (  Exception e) {
    throw new ElasticSearchIllegalArgumentException("failed to build synonyms",e);
  }
}
