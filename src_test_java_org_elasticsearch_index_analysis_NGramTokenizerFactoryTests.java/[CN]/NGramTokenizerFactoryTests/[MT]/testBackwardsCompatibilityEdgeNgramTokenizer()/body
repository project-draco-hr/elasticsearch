{
  int iters=scaledRandomIntBetween(20,100);
  final Index index=new Index("test");
  final String name="ngr";
  for (int i=0; i < iters; i++) {
    Version v=randomVersion(random());
    if (v.onOrAfter(Version.V_0_90_2)) {
      Builder builder=newAnalysisSettingsBuilder().put("min_gram",2).put("max_gram",3).put("token_chars","letter,digit");
      boolean compatVersion=false;
      if ((compatVersion=random().nextBoolean())) {
        builder.put("version","4." + random().nextInt(3));
        builder.put("side","back");
      }
      Settings settings=builder.build();
      Settings indexSettings=newAnalysisSettingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED,v.id).build();
      Tokenizer edgeNGramTokenizer=new EdgeNGramTokenizerFactory(index,indexSettings,name,settings).create(new StringReader("foo bar"));
      if (compatVersion) {
        assertThat(edgeNGramTokenizer,instanceOf(Lucene43EdgeNGramTokenizer.class));
      }
 else {
        assertThat(edgeNGramTokenizer,instanceOf(EdgeNGramTokenizer.class));
      }
    }
 else {
      Settings settings=newAnalysisSettingsBuilder().put("min_gram",2).put("max_gram",3).put("side","back").build();
      Settings indexSettings=newAnalysisSettingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED,v.id).build();
      Tokenizer edgeNGramTokenizer=new EdgeNGramTokenizerFactory(index,indexSettings,name,settings).create(new StringReader("foo bar"));
      assertThat(edgeNGramTokenizer,instanceOf(Lucene43EdgeNGramTokenizer.class));
    }
  }
  Settings settings=newAnalysisSettingsBuilder().put("min_gram",2).put("max_gram",3).put("side","back").build();
  Settings indexSettings=newAnalysisSettingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED,Version.CURRENT).build();
  try {
    new EdgeNGramTokenizerFactory(index,indexSettings,name,settings).create(new StringReader("foo bar"));
    fail("should fail side:back is not supported anymore");
  }
 catch (  ElasticsearchIllegalArgumentException ex) {
  }
}
