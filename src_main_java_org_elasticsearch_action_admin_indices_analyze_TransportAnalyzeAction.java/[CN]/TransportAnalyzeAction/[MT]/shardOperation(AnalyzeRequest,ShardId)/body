{
  IndexService indexService=null;
  if (shardId != null) {
    indexService=indicesService.indexServiceSafe(shardId.getIndex());
  }
  AnalyzeContext context=parseSource(request);
  Analyzer analyzer=null;
  boolean closeAnalyzer=false;
  String field=null;
  if (context.field() != null) {
    if (indexService == null) {
      throw new ElasticsearchIllegalArgumentException("No index provided, and trying to analyzer based on a specific field which requires the index parameter");
    }
    FieldMapper<?> fieldMapper=indexService.mapperService().smartNameFieldMapper(context.field());
    if (fieldMapper != null) {
      if (fieldMapper.isNumeric()) {
        throw new ElasticsearchIllegalArgumentException("Can't process field [" + context.field() + "], Analysis requests are not supported on numeric fields");
      }
      analyzer=fieldMapper.indexAnalyzer();
      field=fieldMapper.names().indexName();
    }
  }
  if (field == null) {
    if (indexService != null) {
      field=indexService.queryParserService().defaultField();
    }
 else {
      field=AllFieldMapper.NAME;
    }
  }
  if (analyzer == null && context.analyzer() != null) {
    if (indexService == null) {
      analyzer=indicesAnalysisService.analyzer(context.analyzer());
    }
 else {
      analyzer=indexService.analysisService().analyzer(context.analyzer());
    }
    if (analyzer == null) {
      throw new ElasticsearchIllegalArgumentException("failed to find analyzer [" + context.analyzer() + "]");
    }
  }
 else   if (context.tokenizer() != null) {
    TokenizerFactory tokenizerFactory;
    if (indexService == null) {
      TokenizerFactoryFactory tokenizerFactoryFactory=indicesAnalysisService.tokenizerFactoryFactory(context.tokenizer());
      if (tokenizerFactoryFactory == null) {
        throw new ElasticsearchIllegalArgumentException("failed to find global tokenizer under [" + context.tokenizer() + "]");
      }
      tokenizerFactory=tokenizerFactoryFactory.create(context.tokenizer(),DEFAULT_SETTINGS);
    }
 else {
      tokenizerFactory=indexService.analysisService().tokenizer(context.tokenizer());
      if (tokenizerFactory == null) {
        throw new ElasticsearchIllegalArgumentException("failed to find tokenizer under [" + context.tokenizer() + "]");
      }
    }
    TokenFilterFactory[] tokenFilterFactories=new TokenFilterFactory[0];
    if (context.tokenFilters() != null && context.tokenFilters().isEmpty() == false) {
      tokenFilterFactories=new TokenFilterFactory[context.tokenFilters().size()];
      for (int i=0; i < context.tokenFilters().size(); i++) {
        String tokenFilterName=context.tokenFilters().get(i);
        if (indexService == null) {
          TokenFilterFactoryFactory tokenFilterFactoryFactory=indicesAnalysisService.tokenFilterFactoryFactory(tokenFilterName);
          if (tokenFilterFactoryFactory == null) {
            throw new ElasticsearchIllegalArgumentException("failed to find global token filter under [" + tokenFilterName + "]");
          }
          tokenFilterFactories[i]=tokenFilterFactoryFactory.create(tokenFilterName,DEFAULT_SETTINGS);
        }
 else {
          tokenFilterFactories[i]=indexService.analysisService().tokenFilter(tokenFilterName);
          if (tokenFilterFactories[i] == null) {
            throw new ElasticsearchIllegalArgumentException("failed to find token filter under [" + tokenFilterName + "]");
          }
        }
        if (tokenFilterFactories[i] == null) {
          throw new ElasticsearchIllegalArgumentException("failed to find token filter under [" + tokenFilterName + "]");
        }
      }
    }
    CharFilterFactory[] charFilterFactories=new CharFilterFactory[0];
    if (context.charFilters() != null && context.charFilters().isEmpty() == false) {
      charFilterFactories=new CharFilterFactory[context.charFilters().size()];
      for (int i=0; i < context.charFilters().size(); i++) {
        String charFilterName=context.charFilters().get(i);
        if (indexService == null) {
          CharFilterFactoryFactory charFilterFactoryFactory=indicesAnalysisService.charFilterFactoryFactory(charFilterName);
          if (charFilterFactoryFactory == null) {
            throw new ElasticsearchIllegalArgumentException("failed to find global char filter under [" + charFilterName + "]");
          }
          charFilterFactories[i]=charFilterFactoryFactory.create(charFilterName,DEFAULT_SETTINGS);
        }
 else {
          charFilterFactories[i]=indexService.analysisService().charFilter(charFilterName);
          if (charFilterFactories[i] == null) {
            throw new ElasticsearchIllegalArgumentException("failed to find token char under [" + charFilterName + "]");
          }
        }
        if (charFilterFactories[i] == null) {
          throw new ElasticsearchIllegalArgumentException("failed to find token char under [" + charFilterName + "]");
        }
      }
    }
    analyzer=new CustomAnalyzer(tokenizerFactory,charFilterFactories,tokenFilterFactories);
    closeAnalyzer=true;
  }
 else   if (analyzer == null) {
    if (indexService == null) {
      analyzer=indicesAnalysisService.analyzer("standard");
    }
 else {
      analyzer=indexService.analysisService().defaultIndexAnalyzer();
    }
  }
  if (analyzer == null) {
    throw new ElasticsearchIllegalArgumentException("failed to find analyzer");
  }
  List<AnalyzeResponse.AnalyzeToken> tokens=Lists.newArrayList();
  TokenStream stream=null;
  try {
    stream=analyzer.tokenStream(field,context.text());
    stream.reset();
    CharTermAttribute term=stream.addAttribute(CharTermAttribute.class);
    PositionIncrementAttribute posIncr=stream.addAttribute(PositionIncrementAttribute.class);
    OffsetAttribute offset=stream.addAttribute(OffsetAttribute.class);
    TypeAttribute type=stream.addAttribute(TypeAttribute.class);
    int position=0;
    while (stream.incrementToken()) {
      int increment=posIncr.getPositionIncrement();
      if (increment > 0) {
        position=position + increment;
      }
      tokens.add(new AnalyzeResponse.AnalyzeToken(term.toString(),position,offset.startOffset(),offset.endOffset(),type.type()));
    }
    stream.end();
  }
 catch (  IOException e) {
    throw new ElasticsearchException("failed to analyze",e);
  }
 finally {
    if (stream != null) {
      try {
        stream.close();
      }
 catch (      IOException e) {
      }
    }
    if (closeAnalyzer) {
      analyzer.close();
    }
  }
  return new AnalyzeResponse(tokens);
}
