{
  IndexService indexService=null;
  if (request.index() != null) {
    indexService=indicesService.indexServiceSafe(request.index());
  }
  Analyzer analyzer=null;
  boolean closeAnalyzer=false;
  String field=null;
  if (request.field() != null) {
    if (indexService == null) {
      throw new ElasticSearchIllegalArgumentException("No index provided, and trying to analyzer based on a specific field which requires the index parameter");
    }
    FieldMapper fieldMapper=indexService.mapperService().smartNameFieldMapper(request.field());
    if (fieldMapper != null) {
      analyzer=fieldMapper.indexAnalyzer();
      field=fieldMapper.names().indexName();
    }
  }
  if (field == null) {
    field="_all";
  }
  if (analyzer == null && request.analyzer() != null) {
    if (indexService == null) {
      analyzer=indicesAnalysisService.analyzer(request.analyzer());
    }
 else {
      analyzer=indexService.analysisService().analyzer(request.analyzer());
    }
    if (analyzer == null) {
      throw new ElasticSearchIllegalArgumentException("failed to find analyzer [" + request.analyzer() + "]");
    }
  }
 else   if (request.tokenizer() != null) {
    TokenizerFactory tokenizerFactory;
    if (indexService == null) {
      TokenizerFactoryFactory tokenizerFactoryFactory=indicesAnalysisService.tokenizerFactoryFactory(request.tokenizer());
      if (tokenizerFactoryFactory == null) {
        throw new ElasticSearchIllegalArgumentException("failed to find global tokenizer under [" + request.tokenizer() + "]");
      }
      tokenizerFactory=tokenizerFactoryFactory.create(request.tokenizer(),ImmutableSettings.Builder.EMPTY_SETTINGS);
    }
 else {
      tokenizerFactory=indexService.analysisService().tokenizer(request.tokenizer());
      if (tokenizerFactory == null) {
        throw new ElasticSearchIllegalArgumentException("failed to find tokenizer under [" + request.tokenizer() + "]");
      }
    }
    TokenFilterFactory[] tokenFilterFactories=new TokenFilterFactory[0];
    if (request.tokenFilters() != null && request.tokenFilters().length > 0) {
      tokenFilterFactories=new TokenFilterFactory[request.tokenFilters().length];
      for (int i=0; i < request.tokenFilters().length; i++) {
        String tokenFilterName=request.tokenFilters()[i];
        if (indexService == null) {
          TokenFilterFactoryFactory tokenFilterFactoryFactory=indicesAnalysisService.tokenFilterFactoryFactory(tokenFilterName);
          if (tokenFilterFactoryFactory == null) {
            throw new ElasticSearchIllegalArgumentException("failed to find global token filter under [" + request.tokenizer() + "]");
          }
          tokenFilterFactories[i]=tokenFilterFactoryFactory.create(tokenFilterName,ImmutableSettings.Builder.EMPTY_SETTINGS);
        }
 else {
          tokenFilterFactories[i]=indexService.analysisService().tokenFilter(tokenFilterName);
          if (tokenFilterFactories[i] == null) {
            throw new ElasticSearchIllegalArgumentException("failed to find token filter under [" + request.tokenizer() + "]");
          }
        }
        if (tokenFilterFactories[i] == null) {
          throw new ElasticSearchIllegalArgumentException("failed to find token filter under [" + request.tokenizer() + "]");
        }
      }
    }
    analyzer=new CustomAnalyzer(tokenizerFactory,new CharFilterFactory[0],tokenFilterFactories);
    closeAnalyzer=true;
  }
 else   if (analyzer == null) {
    if (indexService == null) {
      analyzer=Lucene.STANDARD_ANALYZER;
    }
 else {
      analyzer=indexService.analysisService().defaultIndexAnalyzer();
    }
  }
  if (analyzer == null) {
    throw new ElasticSearchIllegalArgumentException("failed to find analyzer");
  }
  List<AnalyzeResponse.AnalyzeToken> tokens=Lists.newArrayList();
  TokenStream stream=null;
  try {
    stream=analyzer.reusableTokenStream(field,new FastStringReader(request.text()));
    stream.reset();
    CharTermAttribute term=stream.addAttribute(CharTermAttribute.class);
    PositionIncrementAttribute posIncr=stream.addAttribute(PositionIncrementAttribute.class);
    OffsetAttribute offset=stream.addAttribute(OffsetAttribute.class);
    TypeAttribute type=stream.addAttribute(TypeAttribute.class);
    int position=0;
    while (stream.incrementToken()) {
      int increment=posIncr.getPositionIncrement();
      if (increment > 0) {
        position=position + increment;
      }
      tokens.add(new AnalyzeResponse.AnalyzeToken(term.toString(),position,offset.startOffset(),offset.endOffset(),type.type()));
    }
    stream.end();
  }
 catch (  IOException e) {
    throw new ElasticSearchException("failed to analyze",e);
  }
 finally {
    if (stream != null) {
      try {
        stream.close();
      }
 catch (      IOException e) {
      }
    }
    if (closeAnalyzer) {
      analyzer.close();
    }
  }
  return new AnalyzeResponse(tokens);
}
