{
  IndexService indexService=null;
  if (request.getIndex() != null) {
    indexService=indicesService.indexServiceSafe(request.getIndex());
  }
  Analyzer analyzer=null;
  boolean closeAnalyzer=false;
  String field=null;
  if (request.getField() != null) {
    if (indexService == null) {
      throw new ElasticSearchIllegalArgumentException("No index provided, and trying to analyzer based on a specific field which requires the index parameter");
    }
    FieldMapper fieldMapper=indexService.mapperService().smartNameFieldMapper(request.getField());
    if (fieldMapper != null) {
      analyzer=fieldMapper.indexAnalyzer();
      field=fieldMapper.names().indexName();
    }
  }
  if (field == null) {
    if (indexService != null) {
      field=indexService.queryParserService().defaultField();
    }
 else {
      field=AllFieldMapper.NAME;
    }
  }
  if (analyzer == null && request.getAnalyzer() != null) {
    if (indexService == null) {
      analyzer=indicesAnalysisService.analyzer(request.getAnalyzer());
    }
 else {
      analyzer=indexService.analysisService().analyzer(request.getAnalyzer());
    }
    if (analyzer == null) {
      throw new ElasticSearchIllegalArgumentException("failed to find analyzer [" + request.getAnalyzer() + "]");
    }
  }
 else   if (request.getTokenizer() != null) {
    TokenizerFactory tokenizerFactory;
    if (indexService == null) {
      TokenizerFactoryFactory tokenizerFactoryFactory=indicesAnalysisService.tokenizerFactoryFactory(request.getTokenizer());
      if (tokenizerFactoryFactory == null) {
        throw new ElasticSearchIllegalArgumentException("failed to find global tokenizer under [" + request.getTokenizer() + "]");
      }
      tokenizerFactory=tokenizerFactoryFactory.create(request.getTokenizer(),ImmutableSettings.Builder.EMPTY_SETTINGS);
    }
 else {
      tokenizerFactory=indexService.analysisService().tokenizer(request.getTokenizer());
      if (tokenizerFactory == null) {
        throw new ElasticSearchIllegalArgumentException("failed to find tokenizer under [" + request.getTokenizer() + "]");
      }
    }
    TokenFilterFactory[] tokenFilterFactories=new TokenFilterFactory[0];
    if (request.getTokenFilters() != null && request.getTokenFilters().length > 0) {
      tokenFilterFactories=new TokenFilterFactory[request.getTokenFilters().length];
      for (int i=0; i < request.getTokenFilters().length; i++) {
        String tokenFilterName=request.getTokenFilters()[i];
        if (indexService == null) {
          TokenFilterFactoryFactory tokenFilterFactoryFactory=indicesAnalysisService.tokenFilterFactoryFactory(tokenFilterName);
          if (tokenFilterFactoryFactory == null) {
            throw new ElasticSearchIllegalArgumentException("failed to find global token filter under [" + request.getTokenizer() + "]");
          }
          tokenFilterFactories[i]=tokenFilterFactoryFactory.create(tokenFilterName,ImmutableSettings.Builder.EMPTY_SETTINGS);
        }
 else {
          tokenFilterFactories[i]=indexService.analysisService().tokenFilter(tokenFilterName);
          if (tokenFilterFactories[i] == null) {
            throw new ElasticSearchIllegalArgumentException("failed to find token filter under [" + request.getTokenizer() + "]");
          }
        }
        if (tokenFilterFactories[i] == null) {
          throw new ElasticSearchIllegalArgumentException("failed to find token filter under [" + request.getTokenizer() + "]");
        }
      }
    }
    analyzer=new CustomAnalyzer(tokenizerFactory,new CharFilterFactory[0],tokenFilterFactories);
    closeAnalyzer=true;
  }
 else   if (analyzer == null) {
    if (indexService == null) {
      analyzer=Lucene.STANDARD_ANALYZER;
    }
 else {
      analyzer=indexService.analysisService().defaultIndexAnalyzer();
    }
  }
  if (analyzer == null) {
    throw new ElasticSearchIllegalArgumentException("failed to find analyzer");
  }
  List<AnalyzeResponse.AnalyzeToken> tokens=Lists.newArrayList();
  TokenStream stream=null;
  try {
    stream=analyzer.tokenStream(field,new FastStringReader(request.getText()));
    stream.reset();
    CharTermAttribute term=stream.addAttribute(CharTermAttribute.class);
    PositionIncrementAttribute posIncr=stream.addAttribute(PositionIncrementAttribute.class);
    OffsetAttribute offset=stream.addAttribute(OffsetAttribute.class);
    TypeAttribute type=stream.addAttribute(TypeAttribute.class);
    int position=0;
    while (stream.incrementToken()) {
      int increment=posIncr.getPositionIncrement();
      if (increment > 0) {
        position=position + increment;
      }
      tokens.add(new AnalyzeResponse.AnalyzeToken(term.toString(),position,offset.startOffset(),offset.endOffset(),type.type()));
    }
    stream.end();
  }
 catch (  IOException e) {
    throw new ElasticSearchException("failed to analyze",e);
  }
 finally {
    if (stream != null) {
      try {
        stream.close();
      }
 catch (      IOException e) {
      }
    }
    if (closeAnalyzer) {
      analyzer.close();
    }
  }
  return new AnalyzeResponse(tokens);
}
