{
  int successfulShards=0;
  int failedShards=0;
  List<PercolateShardResponse> shardResults=null;
  List<ShardOperationFailedException> shardFailures=null;
  boolean onlyCount=false;
  for (int i=0; i < shardsResponses.length(); i++) {
    Object shardResponse=shardsResponses.get(i);
    if (shardResponse == null) {
    }
 else     if (shardResponse instanceof BroadcastShardOperationFailedException) {
      failedShards++;
      if (shardFailures == null) {
        shardFailures=new ArrayList<>();
      }
      shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException)shardResponse));
    }
 else {
      PercolateShardResponse percolateShardResponse=(PercolateShardResponse)shardResponse;
      successfulShards++;
      if (!percolateShardResponse.isEmpty()) {
        if (shardResults == null) {
          onlyCount=percolateShardResponse.onlyCount();
          shardResults=new ArrayList<>();
        }
        shardResults.add(percolateShardResponse);
      }
    }
  }
  if (shardResults == null) {
    long tookInMillis=Math.max(1,System.currentTimeMillis() - request.startTime);
    PercolateResponse.Match[] matches=request.onlyCount() ? null : PercolateResponse.EMPTY;
    return new PercolateResponse(shardsResponses.length(),successfulShards,failedShards,shardFailures,tookInMillis,matches);
  }
 else {
    PercolatorService.ReduceResult result=null;
    try {
      result=percolatorService.reduce(onlyCount,shardResults,request);
    }
 catch (    IOException e) {
      throw new ElasticsearchException("error during reduce phase",e);
    }
    long tookInMillis=Math.max(1,System.currentTimeMillis() - request.startTime);
    return new PercolateResponse(shardsResponses.length(),successfulShards,failedShards,shardFailures,result.matches(),result.count(),tookInMillis,result.reducedAggregations());
  }
}
