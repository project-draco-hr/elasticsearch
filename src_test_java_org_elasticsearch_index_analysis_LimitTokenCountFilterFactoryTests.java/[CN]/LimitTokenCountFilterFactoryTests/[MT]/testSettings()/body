{
{
    Settings settings=ImmutableSettings.settingsBuilder().put("index.analysis.filter.limit_1.type","limit").put("index.analysis.filter.limit_1.max_token_count",3).put("index.analysis.filter.limit_1.consume_all_tokens",true).build();
    AnalysisService analysisService=AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
    TokenFilterFactory tokenFilter=analysisService.tokenFilter("limit_1");
    String source="the quick brown fox";
    String[] expected=new String[]{"the","quick","brown"};
    Tokenizer tokenizer=new WhitespaceTokenizer();
    tokenizer.setReader(new StringReader(source));
    assertTokenStreamContents(tokenFilter.create(tokenizer),expected);
  }
{
    Settings settings=ImmutableSettings.settingsBuilder().put("index.analysis.filter.limit_1.type","limit").put("index.analysis.filter.limit_1.max_token_count",3).put("index.analysis.filter.limit_1.consume_all_tokens",false).build();
    AnalysisService analysisService=AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
    TokenFilterFactory tokenFilter=analysisService.tokenFilter("limit_1");
    String source="the quick brown fox";
    String[] expected=new String[]{"the","quick","brown"};
    Tokenizer tokenizer=new WhitespaceTokenizer();
    tokenizer.setReader(new StringReader(source));
    assertTokenStreamContents(tokenFilter.create(tokenizer),expected);
  }
{
    Settings settings=ImmutableSettings.settingsBuilder().put("index.analysis.filter.limit_1.type","limit").put("index.analysis.filter.limit_1.max_token_count",17).put("index.analysis.filter.limit_1.consume_all_tokens",true).build();
    AnalysisService analysisService=AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
    TokenFilterFactory tokenFilter=analysisService.tokenFilter("limit_1");
    String source="the quick brown fox";
    String[] expected=new String[]{"the","quick","brown","fox"};
    Tokenizer tokenizer=new WhitespaceTokenizer();
    tokenizer.setReader(new StringReader(source));
    assertTokenStreamContents(tokenFilter.create(tokenizer),expected);
  }
}
